\documentclass{bioinfo}
\copyrightyear{2015}
\pubyear{2015}

\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\rmfamily{\it #1}}
\SetCommentSty{mycommfont}
\SetKwComment{Comment}{$\triangleright$\ }{}

\usepackage{natbib}

\bibliographystyle{apalike}

\begin{document}
\firstpage{1}

\title[Long-read mapping and assembly]{Minimap and miniasm: fast mapping and de novo assembly for noisy long sequences}
\author[Li]{Heng Li}
\address{Broad Institute, 75 Ames Street, Cambridge, MA 02142, USA}
\maketitle

\begin{methods}

\section{Methods}

\subsection{General notations}

Let $\Sigma=\{\mathrm{A},\mathrm{C},\mathrm{G},\mathrm{T}\}$ be the
alphabet of nucleotides. If symbol $a\in\Sigma$, $\overline{a}$ is the
Watson-Crick complement of $a$. For a string $s=a_1a_2\cdots a_n$ over
$\Sigma$, which is also called a \emph{DNA sequence}, its length is $|s|=n$;
its \emph{reverse complement} is $\overline{s}=\overline{a_1a_2\cdots
a_n}=\overline{a}_n\overline{a}_{n-1}\cdots\overline{a}_1$.
For convenience, we define strand function
$\pi:\Sigma^*\times\{0,1\}\to\Sigma^*$ such that $\pi(s,0)=s$ and
$\pi(s,1)=\overline{s}$.

By convention, we call a $k$-long DNA sequence as a $k$-mer. We use the
notation $s^k_i=a_i\cdots a_{i+k-1}$ to denote a $k$-long substring of $s$
starting at $i$.

\subsection{Minimap}

\subsubsection{Overview of algorithm}

Loosely speaking, a $(w,k)$-minimizer \citep{Roberts:2004fv} of a string is the
smallest $k$-mer in a surrounding window of $w$ consecutive $k$-mers. If
sequence $s'$ is a subsequence of $s$, the minimizers of $s'$ will be a subset
of the minimizers of $s$. If $s'$ and $s$ are close, the number of shared
colinear minimizers between them approximately measures their similarity. In
this sense, the set of minimizers of $s$ is a (possibly reduced) representation
of $s$. When we map a query sequence against a set of target sequences, we may
construct a hash table for all target minimizers and then look up for query
minimizers. Large clusters of colinear minimizer matches imply potential
mappings.

%A similar and more popular concept is $(w,k)$-mer, which is used by many
%aligners such as BLAT and SSAHA2. A $(w,k)$-mer of a string $s$ is a $k$-mer
%starting at position $iw$, $i\le\lfloor(|s|-k+1)/w\rfloor$. If we construct a
%hash table for all $(w,k)$-mers of target sequences, we need to look up for
%every query $k$-mer, not just the $(w,k)$-mers of the query -- in contrast, if
%we use minimizers, we only look up for query minimizers, not every $k$-mer.
%Minimizer-based mapping tends to be more efficient.

\subsubsection{Computing minimizers}

\begin{algorithm}[ht]
\DontPrintSemicolon
\footnotesize
\KwIn{Parameter $w$ and $k$ and sequence $s$ with $|s|\ge w+k-1$}
\KwOut{($w$,$k$)-minimizers, their positions and strands}
\BlankLine
\textbf{Function} {\sc MinizerSketch}$(s,w,k)$
\Begin {
	$\mathcal{M}\gets\emptyset$\Comment*[r]{NB: $M$ is a set; no duplicates}
	\For{$i\gets1$ \KwTo $|s|-w-k+1$} {
		$m\gets\infty$\;
		\nl\For (\Comment*[f]{Find the min value}) {$j\gets0$ \KwTo $w-1$} {
			$(u,v)\gets(\phi(s^k_{i+j}),\phi(\overline{s}^k_{i+j}))$\;
			\If (\Comment*[f]{Skip if strand ambiguous}) {$u\not=v$} { 
				$m\gets\min(m,\min(u,v))$\;
			}
		}
		\nl\For (\Comment*[f]{Collect minimizers}) {$j\gets0$ \KwTo $w-1$} {
			$(u,v)\gets(\phi(s^k_{i+j}),\phi(\overline{s}^k_{i+j}))$\;
			\uIf{$u<v$ {\bf and} $u=m$} {
				$\mathcal{M}\gets\mathcal{M}\cup\{(m,i+j,0)\}$\;
			}\ElseIf{$v<u$ {\bf and} $v=m$}{
				$\mathcal{M}\gets\mathcal{M}\cup\{(m,i+j,1)\}$\;
			}
		}
	}
	\Return $M$\;
}
\caption{Compute minimizers}
\end{algorithm}

Let $\phi:\Sigma^k\to\mathbb{Z}$ be a function that maps any $k$-mer to an integer.
Formally, a double-strand $(w,k,\phi)$-minimizer, or simply a minimizer, of a
string $s$, $|s|\ge w+k-1$, is a triple $(h,i,r)$ such that there exists
$\max(1,i-w+1)\le j\le\min(i,|s|-w-k+1)$ which makes
$$
h=\phi(\pi(s_i^k,r))=\min\big\{\phi(\pi(s_{j+p}^k,r')):0\le p<w,r'\in\{0,1\}\big\}
$$
Let $\mathcal{M}(s)$ be the set of minimizers of $s$.  Algorithm~1 gives the
pseudocode to compute $\mathcal{M}(s)$ in $O(w\cdot|s|)$ time.  Our actual
implementation is close to $O(|s|)$ in average case. It uses a queue to cache
the previous minimals and avoids the loops at line~1 and~2 most of time. In
practice, time spent on collecting minimizers is insignificant.

A natural choice of $\phi$ is to let $\phi(\mathrm{A})=0$,
$\phi(\mathrm{C})=1$, $\phi(\mathrm{G})=2$ and $\phi(\mathrm{T})=3$ and for a
$k$-mer $s=a_1\cdots a_k$, define
$$
\phi(s)=\phi(a_1)\times4^{k-1}+\phi(a_2)\times4^{k-2}+\cdots+\phi(a_k)
$$
It always maps a $k$-mer to a distinct integer. A problem with this function is
that ploy-A, which is often highly enriched in genomes, always gets zero, the
smallest value. The function may oversample these non-informative poly-A and
hurts practical performance. To alleviate this issue, we use $\phi'=h\circ\phi$
instead, where $h$ is an invertible integer hash function on $[0,4^k)$ as is
implemented with Algorithm~2 (http://bit.ly/invihgi). The invertibility of $h$
is not critical to the computation of minimizers, but as such $\phi'$ never
maps two distinct $k$-mers to the same $2k$-bit integer, it helps to reduce
hash collisions.

\begin{algorithm}[ht]
\DontPrintSemicolon
\footnotesize
\KwIn{$p$-bit integer $x$}
\KwOut{hashed $p$-bit integer}
\BlankLine
\textbf{Function} {\sc InvertibleHash}$(x,p)$
\Begin {
	$m\gets2^p-1$\;
	$x\gets(\mbox{\tt\char126}x+(x\mbox{\tt\char60\char60}21))\mbox{ \tt\char38}\mbox{ }m$\;
	$x\gets x\mbox{ \tt\char94}\mbox{ }x\mbox{\tt\char62\char62}24$\;
	$x\gets(x+(x\mbox{\tt\char60\char60}3)+(x\mbox{\tt\char60\char60}8))\mbox{ \tt\char38}\mbox{ }m$\;
	$x\gets x\mbox{ \tt\char94}\mbox{ }x\mbox{\tt\char62\char62}14$\;
	$x\gets(x+(x\mbox{\tt\char60\char60}2)+(x\mbox{\tt\char60\char60}4))\mbox{ \tt\char38}\mbox{ }m$\;
	$x\gets x\mbox{ \tt\char94}\mbox{ }x\mbox{\tt\char62\char62}28$\;
	$x\gets(x+(x\mbox{\tt\char60\char60}31))\mbox{ \tt\char38}\mbox{ }m$\;
	\Return $x$\;
}
\caption{Invertible integer hash function}
\end{algorithm}

Note that in a window of $w$ consecutive $k$-mers, there may be more than one
minimizers. Algorithm~1 keeps them all with the loop at line~2. This makes a
minimizer of $s$ always correspond to a minimizer of $\overline{s}$.
\citet{Roberts:2004fv} did not discuss the treatment of such equally good
minimizers.

\subsubsection{Indexing}

\begin{algorithm}[ht]
\DontPrintSemicolon
\footnotesize
\KwIn{Set of target sequences $\mathcal{T}=\{s_1,\ldots,s_T\}$}
\KwOut{Minimizer hash table $\mathcal{H}$}
\BlankLine
\textbf{Function} {\sc Index}$(\mathcal{T},w,k)$
\Begin {
	Initialize $\mathcal{H}$ as an empty hash table\;
	\For{$t\gets1$ \KwTo $T$} {
		$\mathcal{M}\gets${\sc MinizerSketch}$(s_t,w,k)$\;
		\ForEach{$(h,i,r)\in \mathcal{M}$} {
			$\mathcal{H}[h]\gets\mathcal{H}[h]\cup\{(t,i,r)\}$\;
		}
	}
	\Return $\mathcal{H}$\;
}
\caption{Index target sequences}
\end{algorithm}

Algorithm~3 describes target indexing. It keeps minimizers in all target
sequences in a hash table where the key is the minimizer hash and the value is
a set of target sequence index, the position of the minimizer and the strand
(packed into one 64-bit integer).

In implementation, we do not directly insert minimizers to the hash table.
Instead, we append minimizers to an array and sort the array after collecting
all minimizers. The hash table keeps the intervals on the sorted array. This
procedure dramatically reduces heap allocations and cache misses, and is
supposedly faster than direct hash table insertion.

\subsubsection{Mapping}

Given two sequences $s$ and $s'$, we say we find a \emph{minimizer hit}
$(h,x,i,i')$ if there exist $(h,i,r)\in\mathcal{M}(s)$ and
$(h,i',r')\in\mathcal{M}(s')$ with $x=r\oplus r'$ ($\oplus$ is the XOR
operator). Here $h$ is the minimizer hash value, $x$ indicates the relative
strand and $i$ and $i'$ are the positions on the two sequences, respectively.
We say two minimizer hits $(h_1,x,i_1,i'_1)$ and $(h_2,x,i_2,i'_2)$ are
\emph{$\epsilon$-away} if 1) $x=0$ and $|(i_1-i'_1)-(i_2-i'_2)|<\epsilon$
or 2) $x=1$ and $|(i_1+i'_1)-(i_2+i'_2)|<\epsilon$. Intuitively,
$\epsilon$-away hits are approximately colinear within a band of width
$\epsilon$.  Given a set of minimizer hits $\{(h,x,i,i')\}$, we can cluster
$i-i'$ for $x=0$ or $i+i'$ for $x=1$ to identify long colinear matches.

\begin{algorithm}[ht]
\DontPrintSemicolon
\footnotesize
\KwIn{Hash table $\mathcal{H}$ and query sequence $q$}
\KwOut{Print matching query and target intervals}
\BlankLine
\textbf{Function} {\sc Map}$(\mathcal{H},q,w,k,g)$
\Begin {
	Initialize $\mathcal{A}$ as an empty array\;
	$\mathcal{M}\gets${\sc MinizerSketch}$(q,w,k)$\;
	\nl\ForEach (\Comment*[f]{Collect minimizer hits}) {$(h,i,r)\in \mathcal{M}$} {
		\ForEach{$(t,i',r')\in \mathcal{H}[h]$} {
			\uIf (\Comment*[f]{Minimizers on the same strand}) {$r=r'$} {
%				$x\gets t${\tt\char60\char60}32 {\tt\char124} $(|q|-i+i')$
				Append $(t,0,i-i',i')$ to $\mathcal{A}$\;
			} \Else (\Comment*[f]{On different strands}) {
%				$x\gets{\tt1ULL}${\tt\char60\char60}63 {\tt\char124} $t${\tt\char60\char60}32 {\tt\char124} $(i+i')$
				Append $(t,1,i+i',i')$ to $\mathcal{A}$\;
			}
%			Append $(x,t,i',i)$ to $\mathcal{A}$\;
		}
	}
	Sort $\mathcal{A}=[(t,r,c,i')]$ in the order of the four values in tuples\;
	$b\gets1$\;
	\nl\For (\Comment*[f]{Cluster minimizer hits}) {$e=1$ \KwTo $|\mathcal{A}|$} {
		\If{$e=|\mathcal{A}|$ {\bf or} $\mathcal{A}[e+1].t\not=\mathcal{A}[e].t$ {\bf or} $\mathcal{A}[e+1].r\not=\mathcal{A}[e].r$ {\bf or} $\mathcal{A}[e+1].c-\mathcal{A}[e].c>g$} {
			\nl$\mathcal{C}\gets$the maximal colinear subset of $\mathcal{A}[b..e]$\;
			Print the left- and right-most query/target positions in $\mathcal{C}$\;
			$b\gets e+1$\;
		}
	}
}
\caption{Map a query sequence}
\end{algorithm}

Algorithm~4 gives the details of the mapping algorithm. The loop at line~1
collects minimizer hits between the query and all the target sequences. The
loop at line~2 performs a single-linkage clustering to group approximately
colinear hits. Some hits in a cluster may not be colinear because two minimizer
hits within distance $\epsilon$ are always $\epsilon$-away. To fix this issue,
line~3 finds the maximal colinear subset of hits by solving a longest
increasing sequencing problem. This subset is the final mapping result. In
practical implementation, we set thresholds on the size of the subset and the
number of matching bases in the subset to filter poor mappings.

\subsection{Assembly graph}

Two strings $v$ and $w$ may be mapped to each other based on their sequence
similarity. If $v$ can be mapped to a substring of $w$, we say $w$
\emph{contains} $v$. If a suffix of $v$ and a prefix of $w$ can be mapped to
each other, we say $v$ \emph{overlaps} $w$, or written as $v\to w$.
If we regard strings $v$ and $w$ as vertices, the overlap relationship defines
a directed edge between them. The \emph{length} of $v\to w$ equals the length
of $v$'s prefix that does not overlap $w$.

Let $G=(V,E,\ell)$ be a simple graph (i.e. no multi-edges or loops), where $V$ is a
set of DNA sequences (vertices), $E$ a set of overlaps between them (edges) and
$\ell:E\to\Re_+$ is the edge length fuction. $G$ is said to be
\emph{Watson-Crick complete} if i) $\forall v\in V$, $\overline{v}\in V$ and
ii) $\forall v\to w\in E$, $\overline{w}\to\overline{v}\in E$. $G$ is said to
be \emph{containment-free} if any sequence $v$ is not contained in other
sequences in $V$. If $G$ is both Watson-Crick complete and containment-free, it
is an \emph{assembly graph}.

An assembly graph has the same topology as a string graph~\citep{Myers:2005bh},
though the interpretation of the vertex set $V$ is different. In a string
graph, $V$ is the set of the two ends of sequences, not the set of forward and
reverse-complemented sequences.

\subsection{Assembly}

\end{methods}

\bibliography{miniasm}
\end{document}
